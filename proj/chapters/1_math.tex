\chapter{Математика}

В этой главе описаны основные математические понятия, необходимые для правильного понимания как основных, так и продвинутых методов ML. Охвачены: теория вероятностей, классическая и байесовская статистика, некоторые вопросы мат. анализа.


\section{Случайная величина}

Случайной величиной (RV) называется числовая функция $X$, определенная на некотором множестве элементарных исходов $\Omega$ (обычно подмножество $\mathbb{R}$ или $\mathbb{R}^n$), 

$$
X: \Omega\rightarrow\mathbb{R}.
$$

С прикладной точки зрения на RV часто смотрят как на генераторы случайных чисел с заданным распределением.

\textbf{Примеры:}
\begin{itemize}
    \item Рост людей, взятых из некоторой группы.
    \item Цвет фиксированного пикселя изображения, взятого из некоторого множества изображений.
    \item Некоторый признак из датасета ML задачи.
\end{itemize}


\section{Распределение случайной величины}

Если RV принимает дискретное множество значений $x_1,x_2,...$, то она полностью определяется значениями их вероятностей: $p_k=\mathbb{P}(X=x_k)$.

Если множество значений RV не дискретно, то RV может быть описана своей функцией распределения (CDF, Cumulative distribution function): $F(x)=\mathbb{P}(X<x)$.

В большинстве прикладных случаев CDF оказывается дифференцируемой функцией. Производная от CDF называется плотностью распределения случайной величины (PDF, Probability density function): $f(x)=F'(x)$. Таким образом, по определению 

$$
\mathbb{P}(a<X<b)=\int_{a}^{b}f(x)dx.
$$


\section{Выборка}

Выборкой объема $n$ из генеральной совокупности $X$ называется последовательность независимых и распределенных как $X$ случайных величин: 

$$
X_1, X_2, ..., X_n, \quad X_k \sim X
$$

На практике под выборкой понимают конкретные реализации величин $X_k$, то есть последовательность чисел $x_1, x_2, ..., x_n$.


\section{Закон больших чисел}

Закон больших чисел утверждает, что если $X_1, X_2, ..., X_n$ - выборка объема $n$ из генеральной совокупности $X$, то ее среднее с ростом $n$ стабилизируется к среднему значению $X$:

$$
\frac{X_1+X_2+...+X_n}{n} \approx EX, \quad n\rightarrow\infty.
$$ 


\section{Центральная предельная теорема}

Центральная предельная теорема (CLT) является в некотором смысле уточнением закона больших чисел.
В упрощенном варианте она утверждает, что если $X_1, X_2, ..., X_n$ - выборка объема $n$ из генеральной совокупности $X$, то ее распределение ее среднего при больших $n$ очень близко к нормальному,  

$$
\frac{X_1+X_2+...+X_n}{n} \approx N(\mu, \sigma^2/n), \quad \mu=EX, \sigma^2=DX, \quad n\rightarrow\infty.
$$

Заметим, что если совокупность распределена нормально, $X \sim N(\mu, \sigma^2)$, то предыдущая формула обращается в 
точное равенство при любых $n$.


\section{Статистики}

Пусть $X_1, X_2, ..., X_n$ - выборка объема $n$. Статистикой называется произвольная RV, являющаяся функцией выборки:

$$
T = T(X_1, X_2, ..., X_n).
$$

Часто статистикой называют конкретное значение $T(x_1, x_2, ..., x_n)$, полученное на данной реализации $x_1, x_2, ..., x_n$ выборки.

\textbf{Примеры:}
\begin{itemize}
    \item $\bar{X} = (X_1 + X_2 + ... + X_n)/n$ - выборочное среднее.
    \item $X_{(n)} = \max(X_1, X_2, ..., X_n)$ - максимальное значение в выборке.
    \item медиана, перцентили.
\end{itemize}


\section{Bootstrap}


\section{Классический и байесовский подход}


\section{Метод максимального правдоподобия}


\section{Доверительный интервал}


\section{Байесовский доверительный интервал}


\section{Основные дискретные распределения}

$https://medium.com/@srowen/common-probability-distributions-347e6b945ce4$


\section{Основные непрерывные распределения}


\section{Матричные разложения}

...может разделить главу на части...


\section{К-Л дивергенция}

Дивергенция Кульбака-Лейблера (относительная энтропия) - мера сходства двух распределений. Для RV $P$, $Q$ вычисляется как
$$
K(p, q) = -\sum_{i=0}^np_i\log_2\frac{q_i}{p_i}.
$$

Определено для распределений $q_i = 0 \Rightarrow p_i=0$.
В байесовской интерпретации это информация, приобретенная при переходе от априорного распределения $p$ к апостериорному распределению $q$.

К-Л дивергенция всегда неотрицательна и аддитивна в следующем смысле: если $P_1$, $P_2$ - независимые RV, $Q_1$, $Q_2$ - тоже, то для совместных плотностей распределения $P$ и $Q$ справедливо:
$$
K(P, Q) =  K(P_1, Q_1) + K(P_2, Q_2).
$$

К-Л дивергенция может использоваться как метрика между распределениями, так как при $n \rightarrow \infty$ справедливо
$$
K(P_n, Q) \rightarrow 0 \quad \Rightarrow \quad P_n \rightarrow Q
$$
в смысле распределений.

\section{Энтропия}


\section{Кросс-энтропия}


\section{Квантили}


\section{Точечные оценки}


\section{Интервальные оценки}


\section{Проверка гипотез ***}

Пусть $x_1, x_2, ..., x_n$ - выборка из генеральной совокупности $X$, распределение которой заранее неизвестно.
Требуется проверить некоторое утверждение о распределении генеральной совокупности $X$ исходя из имеющейся выборки.

Общий подход к решению таких задач состоит в следующем:
\begin{enumerate}
    \item Формулируется основная гипотеза $H_0$ о распределении $X$ и некоторая альтернативная гипотеза $H_1$, которая может являться полным отрицанием $H_0$ (двусторонняя альтернатива), но не обязательно (односторонние и др. альтернативы).
    \item Выбирается некоторая статистика $T$ исходя из условия, что нулевая гипотеза $H_0$ верна тогда и только тогда, когда распределение $T$ известно: $T \sim F(t|H_0)$ (нулевое распределение статистики). 
    \item Вычисляется значение $t^*$ статистики $T$ на имеющейся выборке. По известному распределению $F$ можно судить, насколько вероятно получить значения~$t^*$.
    \item Выбирается уровень значимости $\alpha$.
    \item Вычисляется достигаемый уровень значимости $p_{value}$ и сравнивается с $\alpha$. Если $p_{value} > \alpha$, то нулевая гипотеза $H_0$ не может быть отвергнута, а если $p_{value} \leqslant \alpha$, гипотеза $H_0$ отвергается в пользу альтернативной $H_1$.
\end{enumerate}

\textbf{Пример:} TODO

Обычно нулевая гипотеза $H_0$ означает, что "ничего интересного не происходит", а альтернативная, напротив, говорит о том, что "что-то произошло".

\section{Достигаемый уровень значимости, $p_{value}$ ***}

Пусть статистика $T$ приняла на выборке $x_1, x_2, ..., x_n$ значение $t^*$. Так как нулевое распределение статистики известно, по значению $t^*$ можно судить, насколько оно характерно для данного распределения. Именно, для случая правосторонней альтернативной определим значение

$$
p_{value} = \mathbb{P}(T \geqslant t^* | H_0).
$$

TODO картинка

Если бы $H_0$ была справедлива, то значение $t^*$ вероятно оказалось бы около матожидания распределения $T$ и, как следствие, $p_{value}$ было бы велико. В противном случае, если $t^*$ оказалось далеко правее среднего, $p_{value}$ мало, и нулевую гипотезу следует отвергнуть в пользу правосторонней альтернативы $H_1$.

Число $p_{value}$ называется достигаемым уровнем значимости. Оно сравнивается с заранее заданным уровнем значимости $\alpha$, и если $p_{value} > \alpha$, то нулевая гипотеза $H_0$ не может быть отвергнута, а если $p_{value} \leqslant \alpha$, гипотеза $H_0$ отвергается в пользу альтернативной $H_1$.

Аналогичные рассуждения справедливы для случаев лево- и двусторонней альтернативы.

\section{Множественная проверка гипотез}

Пусть имеется ряд статистических гипотез, которые следует проверить в совокупности, то есть совокупная нулевая гипотеза состоит в том, что во все исходные нулевые гипотезы верны. Пусть $\alpha$ - уровень значимости для всех исходных гипотез. Опрерировать тем же уровнем значимости для совокупной гипотезы было бы неверно, так как отвергание совокупной нулевой гипотезы означало бы, что хоть одна из исходных гипотез отвергается. Вероятность этого события
$$
1 - (1 - \alpha)^m,
$$
экспоненциально близка к 1 при больших $m$. Таким образом, вероятность совершить ошибку первого рода для множественной проверки гипотез всегда очень близка к 1, если использовать тот же уровень значимости, что и для исходных гипотез.

\textbf{Пример:} Некоторое лекарство исследуется на $m$ возможных побочных эффектов. Чем больше $m$, тем выше вероятность, что хоть один побочный эффект проявится. Это не означает, что этот побочный эффект характерен для лекарства, он мог проявиться просто случайно.

\section{Коррекции на множественную проверку гипотез}

\section{Ошибки I и II рода}


\section{Уровень значимости, $\alpha$}

Уровень значимости $\alpha$ - вероятность ошибки первого рода, то есть вероятность отвергнуть верную нулевую гипотезу.


\section{Мощность статистического критерия}

Пусть $\beta$ - вероятности ошибки второго рода, то есть вероятности принять неверную нулевую гипотезу. Величина $1 - \beta$ называется мощностью статистического критерия. Таким образом, мощность статистического критерия - это вероятность отвергнуть неверную нулевую гипотезу.

\section{Основные задачи статистики}

...из лекций новосиба  курсера...


\section{Параметрические и непараметрические критерии, бутстреп}


\section{Проверка основных гипотез}


\section{Корреляция Пирсона}


\section{Корреляция Спирмена}


\section{Корреляция Метьюса}


\section{Корреляция Крамера}


\section{Z-тест Фишера}


\section{T-тест Стьюдента}


\section{Критерий Пирсона $\chi^2$}


\section{Точный тест Фишера}


\section{Проклятье размерности}



