\chapter{Математика}

В этой главе описаны основные математические понятия, необходимые для правильного понимания как основных, так и продвинутых методов ML. Охвачены: теория вероятностей, классическая и байесовская статистика, некоторые вопросы мат. анализа.


\section{Случайная величина}

Случайной величиной (RV) называется числовая функция $X$, определенная на некотором множестве элементарных исходов $\Omega$ (обычно подмножество $\mathbb{R}$ или $\mathbb{R}^n$), 

$$
X: \Omega\rightarrow\mathbb{R}.
$$

С прикладной точки зрения на RV часто смотрят как на генераторы случайных чисел с заданным распределением.

\textbf{Примеры:}
\begin{itemize}
    \item Рост людей, взятых из некоторой группы.
    \item Цвет фиксированного пикселя изображения, взятого из некоторого множества изображений.
    \item Некоторый признак из датасета ML задачи.
\end{itemize}


\section{Распределение случайной величины}

Если RV принимает дискретное множество значений $x_1,x_2,...$, то она полностью определяется значениями их вероятностей: $p_k=\mathbb{P}(X=x_k)$.

Если множество значений RV не дискретно, то RV может быть описана своей функцией распределения (CDF, Cumulative distribution function): $F(x)=\mathbb{P}(X<x)$.

В большинстве прикладных случаев CDF оказывается дифференцируемой функцией. Производная от CDF называется плотностью распределения случайной величины (PDF, Probability density function): $f(x)=F'(x)$. Таким образом, по определению 

$$
\mathbb{P}(a<X<b)=\int_{a}^{b}f(x)dx.
$$


\section{Выборка}

Выборкой объема $n$ из генеральной совокупности $X$ называется последовательность независимых и распределенных как $X$ случайных величин: 

$$
X_1, X_2, ..., X_n, \quad X_k \sim X
$$

На практике под выборкой понимают конкретные реализации величин $X_k$, то есть последовательность чисел $x_1, x_2, ..., x_n$.


\section{Закон больших чисел}

Закон больших чисел утверждает, что если $X_1, X_2, ..., X_n$ - выборка объема $n$ из генеральной совокупности $X$, то ее среднее с ростом $n$ стабилизируется к среднему значению $X$:

$$
\frac{X_1+X_2+...+X_n}{n} \approx EX, \quad n\rightarrow\infty.
$$ 


\section{Центральная предельная теорема}

Центральная предельная теорема (CLT) является в некотором смысле уточнением закона больших чисел.
В упрощенном варианте она утверждает, что если $X_1, X_2, ..., X_n$ - выборка объема $n$ из генеральной совокупности $X$, то ее распределение ее среднего при больших $n$ очень близко к нормальному,  

$$
\frac{X_1+X_2+...+X_n}{n} \approx N(\mu, \sigma^2/n), \quad \mu=EX, \sigma^2=DX, \quad n\rightarrow\infty.
$$

Заметим, что если совокупность распределена нормально, $X \sim N(\mu, \sigma^2)$, то предыдущая формула обращается в 
точное равенство при любых $n$.


\section{Статистики}

Пусть $X_1, X_2, ..., X_n$ - выборка объема $n$. Статистикой называется произвольная RV, являющаяся функцией выборки:

$$
T = T(X_1, X_2, ..., X_n).
$$

Часто статистикой называют конкретное значение $T(x_1, x_2, ..., x_n)$, полученное на данной реализации $x_1, x_2, ..., x_n$ выборки.

\textbf{Примеры:}
\begin{itemize}
    \item $\bar{X} = (X_1 + X_2 + ... + X_n)/n$ - выборочное среднее.
    \item $X_{(n)} = \max(X_1, X_2, ..., X_n)$ - максимальное значение в выборке.
    \item медиана, перцентили.
\end{itemize}


\section{Bootstrap}


\section{Классический и байесовский подход}


\section{Метод максимального правдоподобия}


\section{Доверительный интервал}


\section{Байесовский доверительный интервал}


\section{Основные дискретные распределения}

$https://medium.com/@srowen/common-probability-distributions-347e6b945ce4$


\section{Основные непрерывные распределения}


\section{Матричные разложения}

...может разделить главу на части...


\section{К-Л дивергенция}


\section{Энтропия}


\section{Кросс-энтропия}


\section{Квантили}


\section{Точечные оценки}


\section{Интервальные оценки}


\section{Проверка гипотез ***}

Пусть $x_1, x_2, ..., x_n$ - выборка из генеральной совокупности $X$, распределение которой заранее неизвестно.
Требуется проверить некоторое утверждение о распределении генеральной совокупности $X$ исходя из имеющейся выборки.

Общий подход к решению таких задач состоит в следующем:
\begin{enumerate}
    \item Формулируется основная гипотеза $H_0$ о распределении $X$ и некоторая альтернативная гипотеза $H_1$, которая может являться полным отрицанием $H_0$ (двусторонняя альтернатива), но не обязательно (односторонние и др. альтернативы).
    \item Выбирается некоторая статистика $T$ исходя из условия, что нулевая гипотеза $H_0$ верна тогда и только тогда, когда распределение $T$ известно: $T \sim F(t|H_0)$ (нулевое распределение статистики). 
    \item Вычисляется значение $t^*$ статистики $T$ на имеющейся выборке. По известному распределению $F$ можно судить, насколько вероятно получить значения~$t^*$.
    \item Выбирается уровень значимости $\alpha$.
    \item Вычисляется достигаемый уровень значимости $p_{value}$ и сравнивается с $\alpha$. Если $p_{value} > \alpha$, то нулевая гипотеза $H_0$ не может быть отвергнута, а если $p_{value} \leqslant \alpha$, гипотеза $H_0$ отвергается в пользу альтернативной $H_1$.
\end{enumerate}

\textbf{Пример:} TODO

Обычно нулевая гипотеза $H_0$ означает, что "ничего интересного не происходит", а альтернативная, напротив, говорит о том, что "что-то произошло".

\section{$p_{value}$ ***}

Пусть статистика $T$ приняла на выборке $x_1, x_2, ..., x_n$ значение $t^*$. Так как нулевое распределение статистики известно, по значению $t^*$ можно судить, насколько оно характерно для данного распределения. Именно, для случая правосторонней альтернативной определим значение

$$
p_{value} = \mathbb{P}(T \geqslant t^* | H_0).
$$

TODO картинка

Если бы $H_0$ была справедлива, то значение $t^*$ вероятно оказалось бы около матожидания распределения $T$ и, как следствие, $p_{value}$ было бы велико. В противном случае, если $t^*$ оказалось далеко правее среднего, $p_{value}$ мало, и нулевую гипотезу следует отвергнуть в пользу правосторонней альтернативы $H_1$.

Число $p_{value}$ называется достигаемым уровнем значимости. Оно сравнивается с заранее заданным уровнем значимости $\alpha$, и если $p_{value} > \alpha$, то нулевая гипотеза $H_0$ не может быть отвергнута, а если $p_{value} \leqslant \alpha$, гипотеза $H_0$ отвергается в пользу альтернативной $H_1$.

Число $p_{value}$ также равно вероятности ошибки первого рода, то есть вероятности отвергнуть верную нулевую гипотезу.

Аналогичные рассуждения справедливы для случаев лево- и двусторонней альтернативы.

\section{Множественная проверка гипотез}


\section{Параметрические и непараметрические критерии, бутстреп}


\section{Ошибки I и II рода}


\section{Достигаемый уровень значимости}


\section{Мощность статистического критерия}


\section{Основные задачи статистики}

...из лекций новосиба  курсера...


\section{Проверка основных гипотез}


\section{Корреляция Пирсона}


\section{Корреляция Спирмена}


\section{Корреляция Метьюса}


\section{Корреляция Крамера}


\section{Z-тест Фишера}


\section{T-тест Стьюдента}


\section{Критерий Пирсона $\chi^2$}


\section{Точный тест Фишера}



